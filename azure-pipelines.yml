resources:
  - repo: self

trigger:
  - main
  - refs/tags/v*

variables:
  python.version: "3.8"
  major_minor: $[format('{0:yy}.{0:MM}', pipeline.startTime)]
  counter_unique_key: $[format('{0}.demo', variables.major_minor)]
  patch: $[counter(variables.counter_unique_key, 0)]
  fallback_tag: $(major_minor).dev$(patch)
  databricks.cluster.id: 
  databricks.host: https://westeurope.azuredatabricks.net
  databricks.cluster.name: single_node_73ML
  library.name:

stages:
  - stage: Test
    jobs:
      - job: Test
        displayName: Test
        steps:
          - task: UsePythonVersion@0
            displayName: "Use Python $(python.version)"
            inputs:
              versionSpec: "$(python.version)"

          # # - task: SonarCloudPrepare@1
          # #   inputs:
          # #     SonarCloud: 'SonarCloud'
          # #     organization: 'pdemeulenaer'
          # #     scannerMode: 'CLI'
          # #     configMode: 'file'
          # #     extraProperties: |
          # #       # Additional properties that will be passed to the scanner, 
          # #       # Put one key=value per line, example:
          # #       # Comma-separated paths to directories with sources (required)
          # #       sonar.sources=.
          # #       sonar.exclusions=htmlcov/**, tests/**/*
          # #       #sonar.inclusion=**/*.py

          # #       # Language
          # #       sonar.language=py

          # #       # Linting
          # #       sonar.python.pylint.reportPaths=pylint_report.txt
          # #       sonar.python.pylint.reportPath=pylint_report.txt

          # #       # Unit tests
          # #       sonar.tests=.
          # #       sonar.test.inclusions=tests/**/*
          # #       #, **/*.py
          # #       sonar.python.xunit.reportPath=junit/test-results.xml
          # #       ##sonar.testExecutionReportPaths=junit/test-results.xml
          # #       ##sonar.testExecutionReportPath=TEST-fake-execution-report.xml
          # #       #sonar.testExecutionReportPaths=TEST-fake-execution-report.xml
          # #       sonar.python.coverage.reportPaths=coverage.xml

          # - task: SonarCloudPrepare@1
          #   inputs:
          #     SonarCloud: 'SonarCloud'
          #     organization: 'pdemeulenaer'
          #     scannerMode: 'CLI'
          #     configMode: 'manual'
          #     cliProjectKey: 'pdemeulenaer_demo-project'
          #     cliProjectName: 'pdemeulenaer_demo-project'
          #     cliSources: '.'
          #     extraProperties: |
          #       # Additional properties that will be passed to the scanner:
          #       sonar.sources=.
          #       sonar.exclusions=htmlcov/**, tests/**/*
          #       #sonar.inclusion=**/*.py
          #       # Language
          #       sonar.language=py
          #       # Encoding
          #       sonar.sourceEncoding=UTF-8
          #       # Linting
          #       sonar.python.pylint.reportPaths=pylint_report.txt
          #       # Unit tests
          #       sonar.tests=.
          #       sonar.test.inclusions=tests/**/*
          #       #, **/*.py
          #       sonar.python.xunit.reportPath=junit/test-results.xml
          #       ##sonar.testExecutionReportPaths=junit/test-results.xml
          #       ##sonar.testExecutionReportPath=TEST-fake-execution-report.xml
          #       #sonar.testExecutionReportPaths=TEST-fake-execution-report.xml
          #       sonar.python.coverage.reportPaths=coverage.xml   

          # - script: pip install pipenv && pipenv install -d --system --deploy --ignore-pipfile
          #   displayName: "Install dependencies"
            
          # - script: "pip install wheel && python setup.py bdist_wheel && pip install -e ."
          #   displayName: "Building the package and install it"

          # # - script: pip install typed_ast && make lint
          # #   displayName: Lint

          # # - script: |
          # #     python -m pip install flake8
          # #     flake8 .
          # #   displayName: 'Run lint flake8 code analysis'
          # #   continueOnError: true

          # - script: |
          #     python -m pip install pylint
          #     #pylint src/ tests/ > pylint_report.txt
          #     pylint src/ tests/ -r n --msg-template="{path}:{line}: [{msg_id}({symbol}), {obj}] {msg}" | tee pylint_report.txt
          #     cat pylint_report.txt
          #   displayName: 'Run lint pylint code analysis'
          #   continueOnError: true            

          # # - script: pip install pathlib2 && make test
          # #   displayName: Test
          # #   continueOnError: true
          
          # #other test capability          
          # - script: |
          #     pip install pytest
          #     pip install pytest-cov
          #     #pytest tests --doctest-modules --junitxml=junit/test-results.xml --cov=. -v --cov-report=xml --cov-report=html
          #     pytest tests/ --doctest-modules --junitxml=junit/test-results.xml --cov=. -v --ignore='venv/*' --cov-config=$(System.DefaultWorkingDirectory)/.coveragerc --cov-report=xml:$(System.DefaultWorkingDirectory)/coverage.xml 
          #   displayName: 'Test with pytest'
          #   continueOnError: true

          # # - task: PublishTestResults@2
          # #   displayName: "Publish Test Results junit/*"
          # #   condition: always()
          # #   inputs:
          # #     testResultsFiles: "junit/*"
          # #     testRunTitle: "Python $(python.version)"

          # - task: PublishTestResults@2
          #   condition: succeededOrFailed()
          #   inputs:
          #     testResultsFiles: '**/test-*.xml'
          #     testRunTitle: 'Publish test results for Python $(python.version)'
              
          # - task: PublishCodeCoverageResults@1
          #   inputs:
          #     codeCoverageTool: Cobertura
          #     summaryFileLocation: '$(System.DefaultWorkingDirectory)/**/coverage.xml'
          #     reportDirectory: '$(System.DefaultWorkingDirectory)/**/htmlcov'              

          # - task: SonarCloudAnalyze@1

          # - task: SonarCloudPublish@1
          #   inputs:
          #     pollingTimeoutSec: '300'

  - stage: Build
    dependsOn: Test
    jobs:
      - job: Build
        displayName: Build
        steps:
          - task: UsePythonVersion@0
            displayName: "Use Python $(python.version)"
            inputs:
              versionSpec: "$(python.version)"

          # - script: "pip install wheel"
          #   displayName: "Wheel"

          # - script: "pip install wheel && python setup.py bdist_wheel && pip install -e ."
          #   displayName: "Building the package and install it"

          - script: |
              # Get version from git tag (v1.0.0) -> (1.0.0)
              git_tag=`git describe --abbrev=0 --tags | cut -d'v' -f 2`
              echo "##vso[task.setvariable variable=git_tag]$git_tag"
            displayName: Set GIT_TAG variable if tag is pushed
            condition: contains(variables['Build.SourceBranch'], 'refs/tags/v')

          - script: |
              # Get variables that are shared across jobs
              GIT_TAG=$(git_tag)
              FALLBACK_TAG=$(fallback_tag)
              echo GIT TAG: $GIT_TAG, FALLBACK_TAG: $FALLBACK_TAG

              # Export variable so python can access it
              export PACKAGE_VERSION=${GIT_TAG:-${FALLBACK_TAG:-default}}
              echo Version used in setup.py: $PACKAGE_VERSION

              # Use PACKAGE_VERSION in setup()
              #python setup.py bdist_wheel
              pip install wheel && python setup.py bdist_wheel && pip install -e .
            displayName: Build the package

          - script: |
              library_name=$(ls ./dist)
              echo "##vso[task.setvariable variable=library.name;]$library_name"
              echo $(library.name)
            displayName: "Extract the name of the package"

          - task: Bash@3
            displayName: 'Extract the name of the package 2'
            inputs:
              targetType: 'inline'
              script: 'echo $(library.name)'

          - task: CopyFiles@2
            displayName: Copy dist files
            inputs:
              sourceFolder: dist/
              contents: demo*.whl
              targetFolder: $(Build.ArtifactStagingDirectory)
              flattenFolders: true

          - task: PublishBuildArtifacts@1
            displayName: PublishArtifact
            inputs:
              pathtoPublish: $(Build.ArtifactStagingDirectory)
              ArtifactName: demo.whl

    # - task: PublishBuildArtifacts@1
    #   inputs:
    #     PathtoPublish: '$(Build.ArtifactStagingDirectory)'
    #     artifactName: 'drop'
    #     publishLocation: 'Container'
    #   displayName: 'Publish documentation as artifact'


  - stage: Deploy
    #dependsOn: Build
    jobs:
      - job: Deploy
        displayName: Deploy
        pool:
          vmImage: 'ubuntu-latest' 
        steps:
          - task: UsePythonVersion@0
            displayName: "Use Python $(python.version)"
            inputs:
              versionSpec: "$(python.version)"
          - task: Bash@3
            displayName: 'Install Databricks CLI'
            inputs:
              targetType: 'inline'
              script: 'pip install -U databricks-cli'

          - task: Bash@3
            displayName: 'Configure Databricks CLI'
            inputs:
              targetType: 'inline'
              script: |
                # # We need to write the pipe the conf into databricks configure --token since
                # # that command only takes inputs from stdin. 
                # conf=`cat << EOM
                # $(databricks.host)
                # $(databricks.token)
                # EOM`
                
                # # For password auth there are three lines expected
                # # hostname, username, password
                # echo "$conf" | databricks configure --token
                databricks configure --token <<EOF
                $(databricks.host)
                $(databricks.token)
                EOF

          - task: Bash@3
            displayName: 'Detect list of clusters'
            inputs:
              targetType: 'inline'
              script: |
                databricks clusters list          

          # - task: configuredatabricks@0
          #   inputs:
          #     url: '$(databricks.host)'
          #     token: '$(databricks.token)'

          - task: DownloadBuildArtifacts@0
            inputs:
              artifactName: demo.whl
              downloadPath: '$(Build.ArtifactStagingDirectory)'
            
          # THIS WORKS FOR WINDOWS ONLY!
          # - task: databricksDeployDBFSFilesTask@0
          #   inputs:
          #     authMethod: 'bearer'
          #     bearerToken: '$(databricks-token)'
          #     region: 'westeurope'
          #     LocalRootFolder: '$(Build.ArtifactStagingDirectory)/demo.whl'
          #     FilePattern: '*.*.whl'
          #     TargetLocation: '/libraries/'

          #     #LocalRootFolder: '$(System.DefaultWorkingDirectory)/_pdemeulenaer.demo-project/demo.whl'
              
          #     # MISSING STEP: INSTALL LIBRARY
          #     # databricks libraries install --cluster-id=0208-153941-tea855 --whl=dbfs:/libraries/demo-0.0.dev0-py3-none-any.whl

          - task: Bash@3
            displayName: 'Copy the library to DBFS'
            inputs:
              targetType: 'inline'
              script: |
                databricks fs rm dbfs:/libraries/$(library.name)
                databricks fs cp -r $(Build.ArtifactStagingDirectory)/demo.whl/ dbfs:/libraries/$(library.name)

          - task: Bash@3
            displayName: 'Install the library'
            inputs:
              targetType: 'inline'
              script: |
                cluster_id=$(databricks clusters list | grep "$(databricks.cluster.name)" | awk '{print $1}')
                echo "##vso[task.setvariable variable=databricks.cluster.id;]$cluster_id"

                # databricks libraries install --cluster-id=$(databricks.cluster.id) --whl=dbfs:/libraries/$(library.name)

                # echo "Checking Cluster State (Cluster ID: $(databricks.cluster.id))..."
                # cluster_state=$(databricks clusters get --cluster-id "$(databricks.cluster.id)" | jq -r ".state")
                # echo "Cluster State: $cluster_state"